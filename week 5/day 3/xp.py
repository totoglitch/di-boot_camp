# -*- coding: utf-8 -*-
"""xp

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b9z1KpUvJCj9SBVkszuDQwBjJ8UUhXFN
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.svm import SVC
import tensorflow as tf
#from keras.wrappers.scikit_learn import KerasClassifier
from keras.models import Sequential
from keras.layers import Dense

from sklearn.preprocessing import StandardScaler

!pip install kaggle

# Upload kaggle.json file (Run this cell and select your kaggle.json file through the file picker)
from google.colab import files
uploaded = files.upload()

# Make directory named kaggle and copy kaggle.json file there
!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/

# Change the permissions of the file
!chmod 600 ~/.kaggle/kaggle.json


!kaggle datasets download -d utkarshx27/heart-disease-diagnosis-dataset

!unzip heart-disease-diagnosis-dataset.zip

df = pd.read_csv('heart.csv')

df.head()

data = pd.read_csv('dataset_heart.csv')

data.head()

y = data['heart disease']
y=y-1
X = data.drop(columns=['heart disease'])


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)


print(X_train.describe())
print(X_test.describe())

""" Exercise 2 : Logistic Regression without Grid Search
Instructions
Use the dataset to build a logistic regression model without using grid search. Split the data into training and testing sets, then train a logistic regression model and evaluate its performance on the test set.


"""

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)

# Evaluate its performance on the test set
y_pred = log_reg.predict(X_test)
print(f"Logistic Regression Accuracy (without Grid Search): {accuracy_score(y_test, y_pred)}")


from sklearn.model_selection import cross_val_score

# Perform cross-validation
cv_scores = cross_val_score(LogisticRegression(), X, y, cv=5)
print(f"Cross-validation accuracy scores: {cv_scores}")
print(f"Mean cross-validation accuracy: {cv_scores.mean()}")

"""Exercise 3 : Logistic Regression with Grid Search
Instructions
Build a logistic regression model using the dataset, but this time, use GridSearchCV to optimize the hyperparameters such as C and penalty.


"""

# Standardize the feature data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define the parameter grid for GridSearchCV
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear']  # because 'liblinear' is suitable for small datasets and supports both L1 and L2 penalties
}

# Build a logistic regression model using GridSearchCV
grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Evaluate the best model
best_log_reg = grid_search.best_estimator_
y_pred = best_log_reg.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Logistic Regression Accuracy (with Grid Search): {accuracy}")

# Print the best hyperparameters
print(f"Best hyperparameters: {grid_search.best_params_}")

"""Exercise 4 : SVM without Grid Search
Instructions
Train a Support Vector Machine (SVM) classifier on the dataset without using grid search. Choose an appropriate kernel and set the hyperparameters manually.


"""

# Train a Support Vector Machine classifier
svm_clf = SVC(kernel='linear', C=1.0)
svm_clf.fit(X_train, y_train)

# Evaluate its performance on the test set
y_pred = svm_clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"SVM Accuracy (without Grid Search): {accuracy:.2f}")

""" Exercise 5 : SVM with Grid Search
Instructions
Implement an SVM classifier on the dataset with GridSearchCV to find the best combination of C, kernel, and gamma hyperparameters.




"""

param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto']
}

# Build the SVM model using GridSearchCV
grid_search = GridSearchCV(SVC(), param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Evaluate the best model
best_svm_clf = grid_search.best_estimator_
y_pred = best_svm_clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"SVM Accuracy (with Grid Search): {accuracy:.2f}")
print(f"Best Parameters: {grid_search.best_params_}")

""" Exercise 6 : XGBoost without Grid Search
Instructions
Use the dataset to train an XGBoost classifier without hyperparameter tuning. Set the hyperparameters manually and justify your choices.


"""

# Absence (1) or presence (2) of heart disease)

xgb_clf = xgb.XGBClassifier(
    learning_rate=0.1,    # Chosen as a conservative learning rate
    n_estimators=100,     # A reasonable number of boosting rounds
    max_depth=3,          # A conservative depth for simplicity and efficiency
    subsample=0.8,        # Use 80% of the data to prevent overfitting
    colsample_bytree=0.8, # Use 80% of features to prevent overfitting
    random_state=42       # For reproducibility
)

xgb_clf.fit(X_train, y_train)

# Evaluate its performance on the test set
y_pred = xgb_clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"XGBoost Accuracy (without Grid Search): {accuracy:.2f}")

# Define the parameter grid
param_grid = {
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 4, 5],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}

# Create an XGBoost classifier
xgb_clf = xgb.XGBClassifier()

# Create the GridSearchCV object
grid_search = GridSearchCV(xgb_clf, param_grid, cv=5, n_jobs=-1, verbose=1)

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best estimator
best_xgb_clf = grid_search.best_estimator_

# 4. Evaluate the model

# Make predictions
y_pred = best_xgb_clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"XGBoost Accuracy with Grid Search: {accuracy}")

"""
ðŸŒŸ Exercise 7 : XGBoost with Grid Search
Instructions
Train an XGBoost classifier on the dataset using GridSearchCV to optimize hyperparameters such as learning_rate, n_estimators, max_depth, etc.

"""

xgb_clf = xgb.XGBClassifier(
    learning_rate=0.1,    # Chosen as a conservative learning rate
    n_estimators=100,     # A reasonable number of boosting rounds
    max_depth=3,          # A conservative depth for simplicity and efficiency
    subsample=0.8,        # Use 80% of the data to prevent overfitting
    colsample_bytree=0.8, # Use 80% of features to prevent overfitting
    random_state=42       # For reproducibility
)

xgb_clf.fit(X_train, y_train)

# Evaluate its performance on the test set
y_pred = xgb_clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"XGBoost Accuracy (without Grid Search): {accuracy:.2f}")

# Define the parameter grid
param_grid = {
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 4, 5],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}

# Create an XGBoost classifier
xgb_clf = xgb.XGBClassifier()

# Create the GridSearchCV object
grid_search = GridSearchCV(xgb_clf, param_grid, cv=5, n_jobs=-1, verbose=1)
# Fit the model
grid_search.fit(X_train, y_train)


# Make predictions
y_pred = best_xgb_clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"XGBoost Accuracy with Grid Search: {accuracy}")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 42)
print('X_train dimension= ', X_train.shape)
print('X_test dimension= ', X_test.shape)
print('y_train dimension= ', y_train.shape)
print('y_train dimension= ', y_test.shape)