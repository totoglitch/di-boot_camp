{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1 : Analyzing Confusion Matrix\n",
        "Instructions\n",
        "Imagine you have a dataset for a binary classification problem, such as email spam detection, where emails are classified as either ‘Spam’ or ‘Not Spam’. You are provided with the confusion matrix results of a classifier.\n",
        "- Define in your own words what True Positives, True Negatives, False Positives, and False Negatives mean in the context of this email spam detection problem.\n",
        "- Given a confusion matrix with specific values for TP, TN, FP, FN, calculate the Accuracy, Precision, Recall, and F1-Score.\n",
        "- Discuss how the classifier’s performance would change with a higher number of False Positives compared to False Negatives, and vice versa.\n",
        "\n"
      ],
      "metadata": {
        "id": "2SVJRViOdQI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "true_positives_definition = \"Emails correctly classified as Spam\"\n",
        "true_negatives_definition = \"Emails correctly classified as Not Spam\"\n",
        "false_positives_definition = \"Non-Spam emails incorrectly classified as Spam\"\n",
        "false_negatives_definition = \"Spam emails incorrectly classified as Not Spam\"\n",
        "\n",
        "# Example Confusion Matrix Values\n",
        "TP, TN, FP, FN = 80, 50, 10, 5  # Example values\n",
        "\n",
        "# Calculating Evaluation Metrics\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "precision = TP / (TP + FP)\n",
        "recall = TP / (TP + FN)\n",
        "f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KG-Ue8VDdUJk",
        "outputId": "3c8ddf4e-8693-4d75-ef83-091b330e336f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.896551724137931, Precision: 0.8888888888888888, Recall: 0.9411764705882353, F1-Score: 0.9142857142857143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Exercise 2 : Evaluating Trade-offs in Metrics\n",
        "Instructions\n",
        "Consider a medical diagnosis application where a model predicts whether patients have a certain disease.\n",
        "- Explain why high recall is more important than high precision in this medical diagnosis context.\n",
        "- Describe a scenario where precision becomes more important than recall.\n",
        "- Discuss the potential consequences of focusing solely on improving accuracy in imbalanced datasets.\n",
        "\n"
      ],
      "metadata": {
        "id": "vZQ50Kb6dniX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recall_importance_medical = \"\"\"\n",
        "High recall is crucial in medical diagnosis as it reduces the number of False Negatives, which are critical in this context. Missing a true case of the disease could be life-threatening.\n",
        "\"\"\"\n",
        "\n",
        "# Scenario where Precision is more important than Recall\n",
        "precision_important_scenario = \"\"\"\n",
        "In email spam detection, precision is more critical as False Positives (legitimate emails marked as spam) can be more problematic than False Negatives (spam emails not detected).\n",
        "\"\"\"\n",
        "\n",
        "# Consequences of focusing solely on Accuracy\n",
        "accuracy_consequences = \"\"\"\n",
        "Focusing only on accuracy in imbalanced datasets can be misleading as the model might simply predict the majority class most of the time, ignoring the minority class which is often of greater interest.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UWRRMI5ueKfP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 3 : Understanding Cross-Validation and Learning Curves\n",
        "Instructions\n",
        "You are working on a project with a large dataset that involves predicting housing prices based on various features.\n",
        "- Explain the difference between K-Fold Cross-Validation and Stratified K-Fold Cross-Validation. Which one would you choose for this task and why?\n",
        "- Describe what learning curves are and how they can help in understanding the performance of your model.\n",
        "- Discuss the implications of underfitting and overfitting as observed from learning curves, and how you might address these issues.\n",
        "\n"
      ],
      "metadata": {
        "id": "_To37FhAePDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Difference between K-Fold and Stratified K-Fold Cross-Validation\n",
        "k_fold_vs_stratified = \"\"\"\n",
        "K-Fold divides the data into 'K' equal parts without considering the distribution of classes, whereas Stratified K-Fold divides data such that each fold maintains the same percentage of samples for each class.\n",
        "\"\"\"\n",
        "\n",
        "# Learning Curves Explanation\n",
        "learning_curves_explanation = \"\"\"\n",
        "Learning curves plot the model's performance on both the training set and validation set over varying levels of training data or complexity. They help in diagnosing problems like overfitting or underfitting.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Sn9UqRH7eSBh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 4 : Impact of Class Imbalance on Model Evaluation\n",
        "Instructions\n",
        "Imagine you are working on a dataset for detecting a rare disease where only 2% of the instances are positive cases (have the disease).\n",
        "- Explain why using accuracy as an evaluation metric might be misleading in this scenario.\n",
        "- Discuss the importance of precision and recall in the context of this imbalanced dataset.\n",
        "- Propose strategies you could use to more effectively evaluate and improve the model’s performance in this scenario, considering the imbalance in the dataset.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q8SO6UdEeSVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Misleading nature of Accuracy in Imbalanced Datasets\n",
        "accuracy_misleading = \"\"\"\n",
        "In datasets with a significant class imbalance, accuracy can be misleading as it can be high even if the model is only predicting the majority class correctly.\n",
        "\"\"\"\n",
        "\n",
        "# Strategies for Evaluating and Improving Model Performance in Imbalanced Datasets\n",
        "imbalance_strategies = \"\"\"\n",
        "Use metrics like Precision, Recall, and F1-Score. Consider using techniques like SMOTE for oversampling the minority class or adjusting class weights in the model.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "fes6JiideVm5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 5 : Role of Threshold Tuning in Classification Models\n",
        "Instructions\n",
        "You are evaluating a binary classification model that predicts whether a bank’s client will default on a loan. The model outputs a probability score between 0 and 1.\n",
        "- Describe how changing the threshold for classifying a positive case (default) from 0.5 to 0.7 might affect the model’s precision and recall.\n",
        "- Discuss the potential consequences of setting the threshold too high or too low in the context of loan default prediction.\n",
        "- Explain how ROC (Receiver Operating Characteristic) curves and AUC (Area Under the Curve) can assist in finding the optimal threshold.\n",
        "\n"
      ],
      "metadata": {
        "id": "YXooKNu7eV3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold_impact = \"\"\"\n",
        "Increasing the threshold from 0.5 to 0.7 for a positive class can increase precision (fewer False Positives) but may decrease recall (more False Negatives).\n",
        "\"\"\"\n",
        "\n",
        "# ROC and AUC in Threshold Determination\n",
        "roc_auc_explanation = \"\"\"\n",
        "ROC curves plot the True Positive Rate against the False Positive Rate at various threshold settings. AUC provides an aggregate measure of performance across all possible thresholds. They help in finding the optimal balance between sensitivity and specificity.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Fd9Rwvu3eZaS"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}