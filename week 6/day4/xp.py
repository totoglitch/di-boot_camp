# -*- coding: utf-8 -*-
"""xp

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/196WWBHGJZhIYMy49BKjOy59U9bkW-QT8

Exercise 1: Calculating Required Sample Size
You are planning an A/B test to evaluate the impact of a new email subject line on the open rate. Based on past data, you expect a small effect size of 0.3 (an increase from 20% to 23% in the open rate). You aim for an 80% chance (power = 0.8) of detecting this effect if it exists, with a 5% significance level (Î± = 0.05).

Calculate the required sample size per group using Pythonâ€™s statsmodels library.
What sample size is needed for each group to ensure your test is properly powered?
"""

from statsmodels.stats.power import TTestIndPower

effect_size = 0.3
alpha = 0.05
power = 0.8

# Calculate sample size
analysis = TTestIndPower()
sample_size = analysis.solve_power(effect_size=effect_size, power=power, alpha=alpha)

print(f"Required sample size per group: {round(sample_size)}")

"""Exercise 2: Understanding the Relationship Between Effect Size and Sample Size
Using the same A/B test setup as in Exercise 1, you want to explore how changing the expected effect size impacts the required sample size.

Calculate the required sample size for the following effect sizes: 0.2, 0.4, and 0.5, keeping the significance level and power the same.
How does the sample size change as the effect size increases? Explain why this happens.



"""

from statsmodels.stats.power import TTestIndPower

alpha = 0.05
power = 0.8

effect_sizes = [0.2, 0.4, 0.5]

for effect_size in effect_sizes:
  analysis = TTestIndPower()
  sample_size = analysis.solve_power(effect_size=effect_size, power=power, alpha=alpha)
  print(f"Required sample size for effect size {effect_size}: {round(sample_size)}")

Okay, let's break down why changing the effect size changes the required sample size in your A/B test example.

**The Relationship**

* **Smaller Effect Size:** When you expect a smaller effect size (a smaller difference between your two groups), you need a larger sample size to detect that difference with the same level of confidence.
* **Larger Effect Size:**  When you expect a larger effect size (a bigger difference between your two groups), you need a smaller sample size to detect that difference with the same level of confidence.

**Why This Happens**

1. **Statistical Power:**  The power of a test is the probability of correctly rejecting the null hypothesis (finding a significant effect) when the alternative hypothesis is true. In your case, the alternative hypothesis is that there is a real difference in the open rate between the two email subject lines.
2. **Noise vs. Signal:** Imagine your data like a signal buried in noise. A larger effect size means your "signal" is stronger relative to the "noise". With a strong signal, you can detect the difference with a smaller sample size because the difference is more apparent. With a weaker signal (smaller effect size), you need a larger sample size to overcome the noise and accurately detect the true effect.
3. **Precision:** A larger sample size provides more precise estimates of the true population parameters. When you're dealing with a small effect, you need more data to achieve the desired level of precision and confidently differentiate it from random fluctuations.

**In Your Code Example**

You can see this relationship in action:

* When the effect size is 0.2, you need a larger sample size (around 395 per group).
* As the effect size increases to 0.4 and 0.5, the required sample size decreases (around 64 and 32 per group respectively).

**In Essence**

The required sample size is a trade-off between the size of the effect you want to detect, the desired level of confidence (power), and the significance level (alpha). If you want to detect a subtle difference (small effect size), you'll need more data to be confident in your findings.  If you anticipate a larger difference, you can get away with a smaller sample size and still achieve adequate power.

I hope this helps clear up the relationship between effect size and sample size in your A/B testing context!

"""Exercise 3: Exploring the Impact of Statistical Power
Imagine you are conducting an A/B test where you expect a small effect size of 0.2. You initially plan for a power of 0.8 but wonder how increasing or decreasing the desired power level impacts the required sample size.

Calculate the required sample size for power levels of 0.7, 0.8, and 0.9, keeping the effect size at 0.2 and significance level at 0.05.
Question: How does the required sample size change with different levels of statistical power? Why is this understanding important when designing A/B tests?



"""

from statsmodels.stats.power import TTestIndPower

effect_size = 0.2
alpha = 0.05

power_levels = [0.7, 0.8, 0.9]

for power in power_levels:
    analysis = TTestIndPower()
    sample_size = analysis.solve_power(effect_size=effect_size, power=power, alpha=alpha)
    print(f"Required sample size for power {power}: {round(sample_size)}")
# it goes up
# because it helps you understand you dont always need more power and it helps you save resorcses

"""
ðŸŒŸ Exercise 4: Implementing Sequential Testing
You are running an A/B test on two versions of a product page to increase the purchase rate. You plan to monitor the results weekly and stop the test early if one version shows a significant improvement.

Define your stopping criteria.
Decide how you would implement sequential testing in this scenario.
At the end of week three, Version B has a p-value of 0.02. What would you do next?
"""

# prompt: run an a/b test on two versions of a product page to increase purchase rate

from statsmodels.stats.power import TTestIndPower
import numpy as np
from scipy import stats


# Example:
#  Let's say we want to detect a 10% increase in purchase rate (relative effect size)
# with a significance level of 0.05 and a power of 0.8.
# Also, we want a maximum number of weeks of 8 weeks

# Parameters
alpha = 0.05  # Type I error rate
beta = 0.20  # Type II error rate (1 - power)
relative_effect_size = 0.10  # Target effect size
max_weeks = 8


# # Function for calculating SPRT boundaries and performing the test
# def sequential_test(data_a, data_b, alpha, beta, relative_effect_size):
#     n_a = len(data_a)
#     n_b = len(data_b)

#     # Calculate the log likelihood ratio for each data point
#     log_likelihood_ratio = np.sum(np.log(stats.binom.pmf(data_b, n_b, (1 + relative_effect_size) / (1 + relative_effect_size)))) - np.sum(np.log(stats.binom.pmf(data_a, n_a, 1 / (1 + relative_effect_size))))

#     # Calculate the SPRT boundaries
#     a = np.log(beta / (1 - alpha))  # Lower boundary
#     b = np.log((1 - beta) / alpha)  # Upper boundary

#     # Check if the log likelihood ratio crosses the boundaries
#     if log_likelihood_ratio <= a:
#         return 'A'  # Accept A as better
#     elif log_likelihood_ratio >= b:
#         return 'B'  # Accept B as better
#     else:
#         return 'Continue'  # Continue the test

# Placeholder data for each week (replace with real data)
# Example: Purchase rates are stored
# versions_a = [10, 15, 20]
# versions_b = [12, 17, 25]

# Week 3 analysis
# p_value = 0.02  # Assume Version B has a p-value of 0.02

# Based on a specified p-value cut-off, you decide whether to accept the null hypothesis (there's no difference) or reject it.
# Here we can set a threshold of 0.05
# Example:
# if p_value <= alpha:
#     print("Significant improvement detected in Version B. Potentially stop the test early and select Version B")
# else:
#     print("No significant improvement detected yet. Continue the test.")


# Based on the SPRT results or the p-value, we can decide our next steps
# For instance, if the p-value at the end of week 3 is below our predefined alpha level (0.05):
# We would have sufficient evidence to reject the null hypothesis that both versions have the same purchase rate
# We may stop the A/B test early and select Version B as the superior choice.
# In case of stopping the test, we would implement the better version of the page, which is in our case Version B

# If there is no significant difference, you would continue the test for more weeks.
# By stopping the test at week 3, we can avoid wasting more time and resources.

def sequential_test(data_a, data_b, alpha, beta, relative_effect_size):
    n_a = len(data_a)
    n_b = len(data_b)

    # Calculate the log likelihood ratio for each data point
    log_likelihood_ratio = np.sum(np.log(stats.binom.pmf(data_b, n_b, (1 + relative_effect_size) / (1 + relative_effect_size)))) - np.sum(np.log(stats.binom.pmf(data_a, n_a, 1 / (1 + relative_effect_size))))

    # Calculate the SPRT boundaries
    a = np.log(beta / (1 - alpha))  # Lower boundary
    b = np.log((1 - beta) / alpha)  # Upper boundary

    # Check if the log likelihood ratio crosses the boundaries
    if log_likelihood_ratio <= a:
        return 'A'  # Accept A as better
    elif log_likelihood_ratio >= b:
        return 'B'  # Accept B as better
    else:
        return 'Continue'  # Continue the test


# Placeholder data for each week (replace with real data)
versions_a = [10, 15, 20]
versions_b = [12, 17, 25]
result = sequential_test(versions_a, versions_b, alpha, beta, relative_effect_size)

if result == "B":
    print("Version B is better, stop the test")
elif result == "A":
    print("Version A is better, stop the test")
else:
    print("Continue the test")

"""Exercise 5: Applying Bayesian A/B Testing
Youâ€™re testing a new feature in your app, and you want to use a Bayesian approach. Initially, you believe the new feature has a 50% chance of improving user engagement. After collecting data, your analysis suggests a 65% probability that the new feature is better.

Describe how you would set up your prior belief.
After collecting data, how does the updated belief (posterior distribution) influence your decision?
What would you do if the posterior probability was only 55%?

"""

import numpy as np
from scipy.stats import beta


prior_alpha = 1
prior_beta = 1

conversions_new = 15
non_conversions_new = 10
conversions_old = 10
non_conversions_old = 15

# Update Belief
# We can update our belief using the observed data to obtain the posterior distribution.
# We do this by adding the number of conversions to the alpha parameter and the number of non-conversions to the beta parameter.

posterior_alpha_new = prior_alpha + conversions_new
posterior_beta_new = prior_beta + non_conversions_new

posterior_alpha_old = prior_alpha + conversions_old
posterior_beta_old = prior_beta + non_conversions_old

# Calculate the probability that the new feature is better
# This is done by calculating the probability that the conversion rate for the new feature is higher than the conversion rate for the old feature.
probability_new_better = 0
for i in np.linspace(0, 1, 1000):
  prob_new = beta.pdf(i, posterior_alpha_new, posterior_beta_new)
  prob_old = beta.pdf(i, posterior_alpha_old, posterior_beta_old)
  if prob_new > prob_old:
    probability_new_better += prob_new/1000

print(f"The probability that the new feature is better is: {probability_new_better:.2f}")

# Influence on Decision:
# - If the posterior probability that the new feature is better is higher than a predefined threshold (e.g., 65%),
# we may decide to launch the new feature.
# - If the posterior probability is only slightly higher than 50% (e.g., 55%), we may be less confident.
# We could continue collecting more data or perform a more thorough analysis to increase our certainty.

# Example:
# If the posterior probability was 65%, we'd be more confident and likely launch the new feature.
# However, if the posterior probability was only 55%, we might choose to collect more data before making a decision.

"""Exercise 6: Implementing Adaptive Experimentation
Youâ€™re running a test with three different website layouts to increase user engagement. Initially, each layout gets 33% of the traffic. After the first week, Layout C shows higher engagement.

Explain how you would adjust the traffic allocation after the first week.
Describe how you would continue to adapt the experiment in the following weeks.
What challenges might you face with adaptive experimentation, and how would you address them?

"""

import numpy as np
from scipy.stats import beta

# Initial traffic allocation (equal for all layouts)
traffic_allocation = [0.33, 0.33, 0.33]

# Placeholder for observed engagement data (replace with actual data)
engagement_rates = [0.10, 0.12, 0.15]  # Example engagement rates for A, B, and C

# Adjust traffic allocation based on observed engagement
def update_traffic_allocation(engagement_rates, traffic_allocation):
    new_allocation = []
    total_engagement = sum(engagement_rates)
    for engagement_rate in engagement_rates:
        new_allocation.append(engagement_rate / total_engagement)
    return new_allocation

# Simulate adaptation over weeks (you'd replace this with a real-world experiment)
for week in range(4):
    print(f"Week {week + 1}:")
    if week > 0:
        traffic_allocation = update_traffic_allocation(engagement_rates, traffic_allocation)

    print(f"Traffic allocation: {traffic_allocation}")
    # Continue collecting data and updating allocation each week...

    # Hypothetical engagement rate improvements (replace with real data)
    engagement_rates = [0.11, 0.13, 0.17]

# Address Challenges of Adaptive Experimentation
# 1. Look-ahead bias: Avoid making decisions based on data that will only be available later.
# 2. Overfitting: Adjust only within bounds to avoid biasing results towards the observed data.
# 3. Frequent peeking: Carefully monitor frequency and use appropriate statistical tests.
# 4. Interpretation complexity: Be cautious when reporting results as early stopping and adjustments impact inference.
# 5. Algorithm complexity: Implement proper validation and ensure robustness and ethical considerations.

# Solutions
# 1. Use robust statistical methods such as multi-armed bandit algorithms that balance exploration and exploitation.
# 2. Establish clear rules for updating allocations to avoid overfitting and biased results.
# 3. Implement robust stopping criteria and carefully consider statistical power and sample size for the experiment.
# 4. Ensure proper reporting methods and carefully define parameters and assumptions before the experiment begins.
# 5. Conduct thorough validation and consider using simpler methods for easy understanding and reproducibility.