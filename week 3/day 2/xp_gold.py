# -*- coding: utf-8 -*-
"""xp gold

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WbLz4wy6rMuBpPWRfBbuH8xnGNegoy0h

Exercise 1: Comparative Analysis of Retail Data
Dataset: Use the Retail Dataset for structured data and Women’s E-Commerce Clothing Reviews for unstructured data.

Analyze the Retail Dataset focusing on sales trends, customer purchase patterns, and store performance.
Analyze the Clothing Reviews dataset to extract insights like predominant sentiments, frequently mentioned topics, and overall customer satisfaction.
Compare the insights you can derive from each dataset and discuss the challenges you faced in processing the unstructured data.
"""

import pandas as pd
# Assuming retail data is in 'retail_data.csv' and reviews in 'reviews.csv'
# Replace with actual file names and paths if needed.

try:
  retail_df = pd.read_csv('retail_data.csv')
  reviews_df = pd.read_csv('reviews.csv')
except FileNotFoundError:
  print("Error: One or both of the data files not found. Please make sure 'retail_data.csv' and 'reviews.csv' are present.")
  exit()


# Retail Data Analysis
print("Retail Data Analysis:")
print("Sales trends:")
# Example: Calculate total sales per month (modify date column name if needed)
# Assuming your retail_data has columns 'Date' and 'Sales'
# retail_df['Date'] = pd.to_datetime(retail_df['Date'])
# retail_df['Month'] = retail_df['Date'].dt.to_period('M')
# monthly_sales = retail_df.groupby('Month')['Sales'].sum()
# print(monthly_sales)
print("Sales trends calculation needs 'Date' and 'Sales' columns. Check your CSV file.")


print("\nCustomer purchase patterns:")
# Example: Identify frequent itemsets using Apriori or FP-Growth algorithms
# This part would require installing a library like mlxtend
# from mlxtend.frequent_patterns import apriori, association_rules
# Note: This is a placeholder, adapt to your specific data and analysis needs
print("Customer purchase patterns analysis requires data preprocessing and potentially external libraries (e.g., mlxtend).")


print("\nStore performance:")
# Example: Compare sales per store or region
# print(retail_df.groupby('Store')['Sales'].sum())
print("Store performance analysis requires a 'Store' column. Check your CSV file.")


# Clothing Reviews Analysis (Unstructured Data)
print("\n\nClothing Reviews Analysis:")

print("Predominant Sentiments:")
# Example: Basic sentiment analysis using textblob
# from textblob import TextBlob
# reviews_df['Sentiment'] = reviews_df['Review Text'].apply(lambda x: TextBlob(x).sentiment.polarity)
# print(reviews_df['Sentiment'].value_counts())
print("Sentiment analysis requires the 'Review Text' column and a sentiment analysis library.")


print("\nFrequently mentioned topics:")
# Use NLP techniques like TF-IDF or topic modeling (e.g. LDA)
print("Topic modeling requires NLP techniques and potentially external libraries (e.g., gensim).")

print("\nOverall Customer Satisfaction:")
# Analyze sentiment scores and ratings
print("Analyzing ratings or review scores gives an idea about overall satisfaction.")


# Comparative Analysis
print("\n\nComparative Analysis:")
print("Comparing trends and insights across datasets is crucial. Use the above insights to draw comparisons.")
print("Challenge with unstructured data: Processing text data is computationally intensive and requires significant data preprocessing and cleaning.")

"""Exercise 2: Basic Data Exploration in E-Commerce
Dataset: Use the “E-Commerce Data” dataset.

Load the dataset using Pandas and print the first few rows to understand its structure.
Print basic information about the dataset, like the number of rows, columns, and column names.
Identify which columns in the dataset represent structured data (like numerical values, dates, fixed categories).
Suggest what type of unstructured data could complement this dataset for a more comprehensive analysis (e.g., customer reviews, product descriptions).
Discuss how this
"""

import pandas as pd

try:
    ecommerce_df = pd.read_csv('E-Commerce Data.csv')
    print(ecommerce_df.head())

    # Basic information about the dataset
    print("\nDataset Information:")
    print("Number of rows:", ecommerce_df.shape[0])
    print("Number of columns:", ecommerce_df.shape[1])
    print("Column names:", ecommerce_df.columns.tolist())

    # Identify structured data columns
    print("\nStructured Data Columns:")
    structured_columns = []
    for col in ecommerce_df.columns:
        if pd.api.types.is_numeric_dtype(ecommerce_df[col]) or pd.api.types.is_datetime64_any_dtype(ecommerce_df[col]) or ecommerce_df[col].dtype == 'object': # Categorical
            structured_columns.append(col)
    print(structured_columns)


    # Suggest complementary unstructured data
    print("\nComplementary Unstructured Data Suggestions:")
    print("Customer reviews: To understand customer sentiment and feedback on products.")
    print("Product descriptions: To gain deeper insights into product features and benefits.")
    print("Social media posts: To gauge public opinion and brand perception.")

except FileNotFoundError:
    print("Error: 'E-Commerce Data.csv' not found. Please make sure the file is present.")
except Exception as e:
    print(f"An error occurred: {e}")

"""Exercise 3: Analyzing a Public Transportation Dataset with a Focus on Data Types
Dataset: Use the “Metro Interstate Traffic Volume” dataset.

Load the dataset and display the first few rows to get a sense of the data.
Identify and print the structured elements in the dataset, such as date-time, traffic volume, etc.
Based on your observation of the dataset, categorize the data elements as structured or unstructured. For example, consider elements like weather descriptions, date-time, and traffic volume. Explain why you categorized them as such.

"""

import pandas as pd
try:
    traffic_df = pd.read_csv('Metro Interstate Traffic Volume.csv')
    print(traffic_df.head())

    print("\nStructured Elements:")
    # Assuming columns like 'date_time', 'traffic_volume', etc., exist.
    # Adjust column names based on the actual dataset.
    structured_cols = ['holiday', 'temp', 'rain_1h', 'snow_1h', 'clouds_all', 'weather_main', 'weather_description', 'date_time', 'traffic_volume']
    for col in structured_cols:
        if col in traffic_df.columns:
          print(f"- {col}: {traffic_df[col].dtype}")
        else:
          print(f"Column '{col}' not found in dataset.")

    print("\nData Element Categorization:")

    # Example categorization (adapt to your dataset)
    print("- date_time: Structured (can be represented as a timestamp)")
    print("- traffic_volume: Structured (numerical data)")
    print("- weather_main: Structured (categorical data, e.g., 'Clear', 'Clouds', 'Rain')")
    print("- weather_description: Semi-structured (text data with some structure; could be further analyzed)")
    print("- temp: Structured (numerical data)")

    # ... categorize other columns similarly

except FileNotFoundError:
    print("Error: 'Metro Interstate Traffic Volume.csv' not found. Please make sure the file is present.")
except Exception as e:
    print(f"An error occurred: {e}")

"""Exercise 4: Basic Data Analysis in a Movie Ratings Dataset
Dataset: Use the “MovieLens Latest Datasets”.

Load the ‘ratings.csv’ file from the dataset, which contains user ratings for movies, and display the first few rows.
Identify and list down the structured elements in the dataset, such as user IDs, movie IDs, ratings, and timestamps.
Explain why the data elements in the ‘ratings.csv’ file are considered structured data.

"""

import pandas as pd
try:
    ratings_df = pd.read_csv('ratings.csv')
    print(ratings_df.head())

    print("\nStructured Elements:")
    structured_elements = ['userId', 'movieId', 'rating', 'timestamp']
    for element in structured_elements:
        print(f"- {element}")

    print("\nExplanation:")
    print("The data elements in 'ratings.csv' are considered structured data because:")
    print("- They have a predefined format and organization.")
    print("- Each column represents a specific attribute (user ID, movie ID, rating, timestamp).")
    print("- The data types are consistent within each column (integer, float, timestamp).")
    print("- The data can be easily organized into a tabular format, making it suitable for relational databases and data analysis techniques.")

except FileNotFoundError:
    print("Error: 'ratings.csv' not found. Please make sure the file is present in the current directory or provide the correct path.")
except Exception as e:
    print(f"An error occurred: {e}")

"""Exercise 5: Creating a Synthetic Product Catalog
We want to generate a synthetic product catalog for an e-commerce platform using Faker.

Ensure that the Faker library is installed and imported into your Python environment.
Create a dataset of 500 products. Each product should have a unique ID, name, description, and price.
Use Pandas to create a DataFrame from the generated data.

"""

import pandas as pd
from faker import Faker

fake = Faker()

products = []
for _ in range(500):
    product = {
        'id': fake.uuid4(),
        'name': fake.catch_phrase(),
        'description': fake.text(),
        'price': fake.pyfloat(min_value=10, max_value=1000)  # Adjust price range as needed
    }
    products.append(product)

# Create a Pandas DataFrame
product_catalog = pd.DataFrame(products)

# Display the first few rows of the DataFrame
print(product_catalog.head())