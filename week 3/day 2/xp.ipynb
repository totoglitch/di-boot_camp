{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ðŸŒŸ Exercise 1: Identifying Data Types\n",
        "Below are various data sources. Identify whether each one is an example of structured or unstructured data.\n",
        "\n",
        "A companyâ€™s financial reports stored in an Excel file.\n",
        "Photographs uploaded to a social media platform.\n",
        "A collection of news articles on a website.\n",
        "Inventory data in a relational database.\n",
        "Recorded interviews from a market research study.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0K9EV3VdazZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A companys financial reports stored in an Excel file. structured\n",
        "Photographs uploaded to a social media platform.  unstructured\n",
        "A collection of news articles on a website. unstructured\n",
        "Inventory data in a relational database. structured\n",
        "Recorded interviews from a market research study. unstructured"
      ],
      "metadata": {
        "id": "XU5T9iu1bfUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 2: Transformation Exercise\n",
        "For each of the following unstructured data sources, propose a method to convert it into structured data. Explain your reasoning.\n",
        "\n",
        "A series of blog posts about travel experiences.\n",
        "Audio recordings of customer service calls.\n",
        "Handwritten notes from a brainstorming session.\n",
        "A video tutorial on cooking.\n"
      ],
      "metadata": {
        "id": "0TiZs_c1cdNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " A series of blog posts about travel experiences.\n",
        " Method: Natural Language Processing (NLP) and topic modeling.\n",
        "Reasoning: NLP techniques can be used to extract key entities (locations, dates, activities), sentiments, and topics from the text.\n",
        "Topic modeling algorithms can group similar blog posts together, revealing recurring themes and patterns.\n",
        "This information can be organized into a structured database with columns for location, date, activity, sentiment, and topic.\n",
        "\n",
        " Audio recordings of customer service calls.\n",
        "Method: Speech-to-text conversion, followed by NLP and sentiment analysis.\n",
        "Reasoning: First, convert audio to text using an accurate speech-to-text engine.\n",
        "Then, employ NLP techniques to identify customer issues, agent responses, and key phrases.\n",
        "Sentiment analysis can determine the emotional tone of the conversation (positive, negative, neutral).\n",
        "Store the extracted information, along with timestamps and call IDs, in a structured database.\n",
        "\n",
        "Handwritten notes from a brainstorming session.\n",
        " Method: Optical Character Recognition (OCR), followed by NLP and keyword extraction.\n",
        " Reasoning: OCR will convert handwritten text to digital text.  NLP techniques can then identify key concepts, ideas, and action items.\n",
        " Keyword extraction will help categorize the notes and highlight important topics. The structured data can include timestamps, author of the note,\n",
        "and a list of keywords extracted from the notes.\n",
        "\n",
        " A video tutorial on cooking.\n",
        " Method: Video transcription (speech-to-text), object recognition, and timestamping.\n",
        "Reasoning: Convert the audio portion of the video to text using a speech-to-text engine.\n",
        "Use object recognition to identify ingredients and cooking utensils shown in the video, along with their timestamps.\n",
        "The structured data could include a list of ingredients, steps in the recipe, timestamps for each step,  and corresponding images/objects identified."
      ],
      "metadata": {
        "id": "7Dtq0Ra1c9J2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸŒŸ Exercise 3 : Import a file from Kaggle\n",
        "Import the train dataset. Use the train.csv file.\n",
        "Print the first few rows of the DataFrame.\n"
      ],
      "metadata": {
        "id": "H7aTenxTeDSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "try:\n",
        "  df = pd.read_csv('train.csv')\n",
        "  print(df.head(5))\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "88Sq3Jx2gUCW",
        "outputId": "8fb67c99-e81a-449f-d0f7-4a1f9f0d7c17"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "3            4         1       1   \n",
            "4            5         0       3   \n",
            "\n",
            "                                                Name     Sex   Age  SibSp  \\\n",
            "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
            "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
            "4                           Allen, Mr. William Henry    male  35.0      0   \n",
            "\n",
            "   Parch            Ticket     Fare Cabin Embarked  \n",
            "0      0         A/5 21171   7.2500   NaN        S  \n",
            "1      0          PC 17599  71.2833   C85        C  \n",
            "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
            "3      0            113803  53.1000  C123        S  \n",
            "4      0            373450   8.0500   NaN        S  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸŒŸ Exercise 4: Importing a CSV File\n",
        "Use the Iris Dataset CSV.\n",
        "\n",
        "Download the Iris dataset CSV file and place it in the same directory as your Jupyter Notebook.\n",
        "Import the CSV file using Pandas.\n",
        "Display the first five rows of the dataset.\n"
      ],
      "metadata": {
        "id": "Ov5kmMYEflut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "try:\n",
        "  df = pd.read_csv('iris.csv')\n",
        "  print(df.head(5))\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hcPcWdvefwSD",
        "outputId": "30400681-13c7-4a8f-b1b4-5084b5e64692"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   5.1  3.5  1.4  0.2  Iris-setosa\n",
            "0  4.9  3.0  1.4  0.2  Iris-setosa\n",
            "1  4.7  3.2  1.3  0.2  Iris-setosa\n",
            "2  4.6  3.1  1.5  0.2  Iris-setosa\n",
            "3  5.0  3.6  1.4  0.2  Iris-setosa\n",
            "4  5.4  3.9  1.7  0.4  Iris-setosa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 5 : Export a dataframe to excel format and JSON format.\n",
        "Create a simple dataframe.\n",
        "Export the dataframe to an excel file.\n",
        "Export the dataframe to a JSON file.\n"
      ],
      "metadata": {
        "id": "KkxLBDZwhjyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'col1': [1, 2, 3, 4], 'col2': [5, 6, 7, 8]}\n",
        "df = pd.DataFrame(data)\n",
        "df.to_excel(\"output.xlsx\", index=False)\n",
        "df.to_json(\"output.json\", orient=\"records\")"
      ],
      "metadata": {
        "id": "b_tQFe0chsw0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 6: Reading JSON Data\n",
        "Use a sample JSON dataset\n",
        "\n",
        "Import the JSON data from the provided URL.\n",
        "Use Pandas to read the JSON data.\n",
        "Display the first five entries of the data.\n"
      ],
      "metadata": {
        "id": "PZHTUP6iiH-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import requests\n",
        "  url = \"https://jsonplaceholder.typicode.com/posts\"\n",
        "  response = requests.get(url)\n",
        "  posts = response.json()\n",
        "  # Display the first five entries\n",
        "  print(posts[:5])\n",
        "# prompt: display the firs 5 entries of the posts.json\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5jemM3DifUz",
        "outputId": "0541dc79-40fb-47bd-8d9a-5a5edb0bad3d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'userId': 1, 'id': 1, 'title': 'sunt aut facere repellat provident occaecati excepturi optio reprehenderit', 'body': 'quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto'}, {'userId': 1, 'id': 2, 'title': 'qui est esse', 'body': 'est rerum tempore vitae\\nsequi sint nihil reprehenderit dolor beatae ea dolores neque\\nfugiat blanditiis voluptate porro vel nihil molestiae ut reiciendis\\nqui aperiam non debitis possimus qui neque nisi nulla'}, {'userId': 1, 'id': 3, 'title': 'ea molestias quasi exercitationem repellat qui ipsa sit aut', 'body': 'et iusto sed quo iure\\nvoluptatem occaecati omnis eligendi aut ad\\nvoluptatem doloribus vel accusantium quis pariatur\\nmolestiae porro eius odio et labore et velit aut'}, {'userId': 1, 'id': 4, 'title': 'eum et est occaecati', 'body': 'ullam et saepe reiciendis voluptatem adipisci\\nsit amet autem assumenda provident rerum culpa\\nquis hic commodi nesciunt rem tenetur doloremque ipsam iure\\nquis sunt voluptatem rerum illo velit'}, {'userId': 1, 'id': 5, 'title': 'nesciunt quas odio', 'body': 'repudiandae veniam quaerat sunt sed\\nalias aut fugiat sit autem sed est\\nvoluptatem omnis possimus esse voluptatibus quis\\nest aut tenetur dolor neque'}]\n"
          ]
        }
      ]
    }
  ]
}